{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bfb09d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set PYTHONPATH environment variable for the kernel\n",
    "robofin_path = os.path.join(os.getcwd(), 'robofin')\n",
    "current_pythonpath = os.environ.get('PYTHONPATH', '')\n",
    "if robofin_path not in current_pythonpath:\n",
    "    os.environ['PYTHONPATH'] = f\"{robofin_path}:{current_pythonpath}\" if current_pythonpath else robofin_path\n",
    "\n",
    "# Also add to sys.path for immediate effect\n",
    "if robofin_path not in sys.path:\n",
    "    sys.path.insert(0, robofin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a73a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(obj):\n",
    "    \"\"\"\n",
    "    Function for displaying nested types.\n",
    "    \n",
    "    e.g. get_type(dict_str_float) -> \"dict[str, float]\"\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        if not obj:\n",
    "            return \"dict[?, ?]\"\n",
    "        key_types = {get_type(k) for k in obj.keys()}\n",
    "        value_types = {get_type(v) for v in obj.values()}\n",
    "        return f\"dict[{', '.join(key_types)}, {', '.join(value_types)}]\"\n",
    "    elif isinstance(obj, list):\n",
    "        if not obj:\n",
    "            return \"list[?]\"\n",
    "        elem_types = {get_type(elem) for elem in obj}\n",
    "        return f\"list[{', '.join(elem_types)}]\"\n",
    "    elif isinstance(obj, tuple):\n",
    "        if not obj:\n",
    "            return \"tuple[?]\"\n",
    "        elem_types = [get_type(elem) for elem in obj]\n",
    "        return f\"tuple[{', '.join(elem_types)}]\"\n",
    "    elif isinstance(obj, set):\n",
    "        if not obj:\n",
    "            return \"set[?]\"\n",
    "        elem_types = {get_type(elem) for elem in obj}\n",
    "        return f\"set[{', '.join(elem_types)}]\"\n",
    "    else:\n",
    "        return type(obj).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ac890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df16f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from robofin.robots import Robot\n",
    "\n",
    "# Load the Robot class with the standard URDF file (that uses relative filepaths)\n",
    "robot = Robot(\"assets/panda/panda.urdf\")\n",
    "# robot = Robot(\"assets/gp7/gp7.urdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae8f8b",
   "metadata": {},
   "source": [
    "## Dataloader/Replay buffer testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "118b7334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from avoid_everything.data_loader import DataModule\n",
    "\n",
    "training_model_parameters = {\n",
    "  \"robot_dof\": 7,  # robot's degrees of freedom\n",
    "  \"point_match_loss_weight\": 1.0,  # point match loss (BC loss) weight for actor\n",
    "  \"actor_loss_weight\": 1.0,        # RL actor loss weight\n",
    "  \"collision_loss_weight\": 1.0,    # collision loss weight (only used for validation right now)\n",
    "  \"collision_loss_margin\": 0.03,   # margin for collision loss [m]\n",
    "  \"min_lr\": 1.0e-5,\n",
    "  \"max_lr\": 5.0e-5,\n",
    "  \"warmup_steps\": 5000,   # number of steps to warm up the learning rate linearly from min_lr to max_lr\n",
    "  \"weight_decay\": 1e-4,   # weight decay for AdamW optimizers\n",
    "  \"gamma\": 0.99,          # discount factor for Q-learning\n",
    "  \"tau\": 0.005,           # target network's soft update rate\n",
    "  \"grad_clip_norm\": 1.0,  # gradient clipping norm\n",
    "  \"pc_bounds\": [[-1.5, -1.5, -0.1], [1.5, 1.5, 1.5]],\n",
    "  \"rollout_length\": 69   # number of steps to rollout for the actor\n",
    "}\n",
    "data_module_parameters = {\n",
    "    \"data_dir\": \"/workspace/datasets/ae_aristotle1_5mm_cubbies\",\n",
    "    \"train_trajectory_key\": \"global_solutions\",\n",
    "    \"val_trajectory_key\": \"global_solutions\",\n",
    "    \"num_obstacle_points\": 4096,\n",
    "    \"random_scale\": 0.015,\n",
    "    \"include_reward\": True,\n",
    "    \"num_target_points\": 128,\n",
    "}\n",
    "shared_parameters = {\n",
    "    \"urdf_path\": \"assets/panda/panda.urdf\",\n",
    "    \"num_robot_points\": 2048,\n",
    "    \"goal_reward\": 100.0,      # reward for reaching the goal\n",
    "    \"collision_reward\": -10.0, # reward for colliding with an obstacle\n",
    "    \"step_reward\": -1.0        # reward for each step that doesn't terminate the episode\n",
    "}\n",
    "cfg = {\n",
    "    \"train_batch_size\": 12,\n",
    "    \"val_batch_size\": 12,\n",
    "    \"num_workers\": 4,\n",
    "    \"expert_fraction\": 0.25\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eecc4cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mLoaded StateRewardDataset for training\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dm = DataModule(\n",
    "    train_batch_size=cfg[\"train_batch_size\"],\n",
    "    val_batch_size=cfg[\"val_batch_size\"],\n",
    "    num_workers=cfg[\"num_workers\"],\n",
    "    **data_module_parameters,\n",
    "    **shared_parameters,\n",
    ")\n",
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ea0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avoid_everything.col.replay import ReplayBuffer\n",
    "\n",
    "replay_buffer = ReplayBuffer(\n",
    "    capacity=1000,\n",
    "    urdf_path=shared_parameters[\"urdf_path\"],\n",
    "    num_robot_points=shared_parameters[\"num_robot_points\"],\n",
    "    num_target_points=data_module_parameters[\"num_target_points\"],\n",
    "    dataset=dm.data_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7dd869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "from lightning.fabric import Fabric\n",
    "from avoid_everything.col.col import CoLMotionPolicyTrainer\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "trainer = CoLMotionPolicyTrainer(\n",
    "    replay_buffer=replay_buffer,\n",
    "    **shared_parameters,\n",
    "    **training_model_parameters,\n",
    ")\n",
    "fabric = Fabric(accelerator=\"gpu\", devices=1)\n",
    "fabric.launch()\n",
    "\n",
    "expert_loader = fabric.setup_dataloaders(dm.train_dataloader(), move_to_device=False)\n",
    "\n",
    "opt_cfg = trainer.configure_optimizers()\n",
    "actor_optim  = opt_cfg[\"actor_optim\"]\n",
    "critic_optim = opt_cfg[\"critic_optim\"]\n",
    "actor_sch    = opt_cfg[\"actor_scheduler\"]\n",
    "critic_sch   = opt_cfg[\"critic_scheduler\"]\n",
    "\n",
    "# fabric setup: wrap trainable modules w/ their optimizers\n",
    "trainer.actor,  actor_optim  = fabric.setup(trainer.actor,  actor_optim)\n",
    "trainer.critic, critic_optim = fabric.setup(trainer.critic, critic_optim)\n",
    "\n",
    "# target networks have no optimizers\n",
    "trainer.target_actor  = fabric.setup(trainer.target_actor)\n",
    "trainer.target_critic = fabric.setup(trainer.target_critic)\n",
    "\n",
    "# now that actor/critic are on the right device, initialize trainer\n",
    "trainer.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54547d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avoid_everything.col.mixed_batch_provider import MixedBatchProvider\n",
    "mixed_provider = MixedBatchProvider(\n",
    "    expert_loader=expert_loader, agent_replay=replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2bd5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expert data loader iterations: 1\n"
     ]
    }
   ],
   "source": [
    "mixed, data_loader_iterations = mixed_provider.sample(\n",
    "    cfg[\"train_batch_size\"],\n",
    "    expert_fraction=cfg[\"expert_fraction\"],\n",
    "    pretraining=True,\n",
    ")\n",
    "batch = trainer.move_batch_to_device(mixed, fabric.device)\n",
    "batch[\"configuration\"].shape\n",
    "print(\"expert data loader iterations:\", data_loader_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f34e8d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean episode reward: -13.666666984558105\n",
      "Transitions collected: 56\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.agent_rollout(batch)\n",
    "print(f\"Mean episode reward: {metrics['avg_episode_reward']}\")\n",
    "print(f\"Transitions collected: {metrics['transitions_collected']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "924ba331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d8ad20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expert data loader iterations: 1\n"
     ]
    }
   ],
   "source": [
    "# sample 25/75 | expert/agent\n",
    "mixed, data_loader_iterations = mixed_provider.sample(\n",
    "    cfg[\"train_batch_size\"],\n",
    "    expert_fraction=cfg[\"expert_fraction\"],\n",
    "    pretraining=False,\n",
    ")\n",
    "batch = trainer.move_batch_to_device(mixed, fabric.device)\n",
    "batch[\"configuration\"].shape\n",
    "print(\"expert data loader iterations:\", data_loader_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def convert_to_numpy_f32(arr: np.ndarray | torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a NumPy array or Torch tensor to a NumPy float32 array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray or torch.Tensor\n",
    "        Input array to convert.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Converted array with dtype float32.\n",
    "    \"\"\"\n",
    "    if isinstance(arr, torch.Tensor):\n",
    "        np_arr: np.ndarray = arr.cpu().numpy()\n",
    "    elif isinstance(arr, np.ndarray):\n",
    "        np_arr: np.ndarray = arr\n",
    "    else:\n",
    "        raise TypeError(\"convert_to_numpy_f32: Input must be a NumPy array or Torch tensor\")\n",
    "    return np_arr.astype(np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2622ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuboid_dims = convert_to_numpy_f32(sample[\"cuboid_dims\"])\n",
    "cuboid_centers = convert_to_numpy_f32(sample[\"cuboid_centers\"])\n",
    "cuboid_quaternions = convert_to_numpy_f32(sample[\"cuboid_quats\"])\n",
    "for dims, center, quat in zip(cuboid_dims, cuboid_centers, cuboid_quaternions):\n",
    "    print(f\"Lengths: Dimensions: {len(dims)}, Center: {len(center)}, Quaternion: {len(quat)}\")\n",
    "\n",
    "cylinder_radii = convert_to_numpy_f32(sample[\"cylinder_radii\"])\n",
    "cylinder_heights = convert_to_numpy_f32(sample[\"cylinder_heights\"])\n",
    "cylinder_centers = convert_to_numpy_f32(sample[\"cylinder_centers\"])\n",
    "cylinder_quaternions = convert_to_numpy_f32(sample[\"cylinder_quats\"])\n",
    "for radius, height, center, quat in zip(cylinder_radii, cylinder_heights, cylinder_centers, cylinder_quaternions):\n",
    "    print(f\"Lengths: Radius: {len(radius)}, Height: {len(height)}, Center: {len(center)}, Quaternion: {len(quat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import viz_client\n",
    "\n",
    "urdf_path = \"/workspace/assets/panda/panda_spheres.urdf\"\n",
    "if not os.path.exists(urdf_path):\n",
    "    print(f\"❌ URDF not found at {urdf_path}\")\n",
    "    print(\"Please update the urdf_path variable in test_connect()\")\n",
    "    raise FileNotFoundError(f\"URDF not found at {urdf_path}\")\n",
    "\n",
    "viz_client.connect(urdf_path)\n",
    "viz_client.publish_joints(joints=robot.neutral_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d165a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_client.publish_obstacles(cuboid_centers=cuboid_centers,\n",
    "                            cuboid_dims=cuboid_dims,\n",
    "                            cuboid_quaternions=cuboid_quaternions,\n",
    "                            cylinder_centers=cylinder_centers,\n",
    "                            cylinder_radii=cylinder_radii,\n",
    "                            cylinder_heights=cylinder_heights,\n",
    "                            cylinder_quaternions=cylinder_quaternions,\n",
    "                            color=[0.8, 0.5, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_client.clear_obstacles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3271b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_client.publish_joints(joints=robot.neutral_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3905d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ad186",
   "metadata": {},
   "source": [
    "## Sampler testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robofin.old.kinematics.numba import franka_arm_link_fk, franka_arm_visual_fk, franka_eef_visual_fk\n",
    "from robofin.old.kinematics.torch import franka_arm_link_fk as torch_franka_arm_link_fk\n",
    "from robofin.old.kinematics.torch import franka_arm_visual_fk as torch_franka_arm_visual_fk\n",
    "\n",
    "fk = robot.fk(robot.neutral_config)\n",
    "\n",
    "prismatic_joint = robot.auxiliary_joint_defaults[\"panda_finger_joint1\"]\n",
    "base_pose = np.eye(4)\n",
    "franka_fk = franka_arm_link_fk(robot.neutral_config, prismatic_joint, base_pose)\n",
    "franka_fk_dict = {\n",
    "    \"panda_link0\": franka_fk[0],\n",
    "    \"panda_link1\": franka_fk[1],\n",
    "    \"panda_link2\": franka_fk[2],\n",
    "    \"panda_link3\": franka_fk[3],\n",
    "    \"panda_link4\": franka_fk[4],\n",
    "    \"panda_link5\": franka_fk[5],\n",
    "    \"panda_link6\": franka_fk[6],\n",
    "    \"panda_link7\": franka_fk[7],\n",
    "    \"panda_link8\": franka_fk[8],\n",
    "    \"panda_hand\": franka_fk[9],\n",
    "    \"panda_grasptarget\": franka_fk[10],\n",
    "    \"right_gripper\": franka_fk[11],\n",
    "    \"panda_leftfinger\": franka_fk[12],\n",
    "    \"panda_rightfinger\": franka_fk[13],\n",
    "}\n",
    "\n",
    "torch_cfg = torch.Tensor(robot.neutral_config).unsqueeze(0)\n",
    "torch_fk = robot.fk_torch(torch_cfg)\n",
    "\n",
    "torch_base_pose = torch.Tensor(base_pose)\n",
    "torch_franka_fk = torch_franka_arm_link_fk(torch_cfg, prismatic_joint, torch_base_pose)\n",
    "torch_franka_fk_dict = {\n",
    "    \"panda_link0\": torch_franka_fk[:,0],\n",
    "    \"panda_link1\": torch_franka_fk[:,1],\n",
    "    \"panda_link2\": torch_franka_fk[:,2],\n",
    "    \"panda_link3\": torch_franka_fk[:,3],\n",
    "    \"panda_link4\": torch_franka_fk[:,4],\n",
    "    \"panda_link5\": torch_franka_fk[:,5],\n",
    "    \"panda_link6\": torch_franka_fk[:,6],\n",
    "    \"panda_link7\": torch_franka_fk[:,7],\n",
    "    \"panda_link8\": torch_franka_fk[:,8],\n",
    "    \"panda_hand\": torch_franka_fk[:,9],\n",
    "    \"panda_grasptarget\": torch_franka_fk[:,10],\n",
    "    \"right_gripper\": torch_franka_fk[:,11],\n",
    "    \"panda_leftfinger\": torch_franka_fk[:,12],\n",
    "    \"panda_rightfinger\": torch_franka_fk[:,13],\n",
    "}\n",
    "\n",
    "print(torch_fk[\"panda_hand\"].shape)\n",
    "print(torch_franka_fk_dict[\"panda_hand\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robofin.samplers import NumpyRobotSampler, TorchRobotSampler\n",
    "from robofin.old.samplers import NumpyFrankaSampler, TorchFrankaSampler\n",
    "\n",
    "np_sampler = NumpyRobotSampler(robot)\n",
    "torch_sampler = TorchRobotSampler(robot)\n",
    "\n",
    "np_franka_sampler = NumpyFrankaSampler()\n",
    "torch_franka_sampler = TorchFrankaSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "franka_ee_pts = np_franka_sampler.sample_end_effector(fk[robot.tcp_link_name].squeeze(), prismatic_joint)\n",
    "ee_pts = np_sampler.sample_end_effector(fk[robot.tcp_link_name].squeeze())\n",
    "\n",
    "print(\"---\" * 10)\n",
    "print(f\"Franka EE points shape: {franka_ee_pts.shape}\")\n",
    "print(f\"EE points shape: {ee_pts.shape}\")\n",
    "\n",
    "torch_franka_ee_pts = torch_franka_sampler.sample_end_effector(torch_franka_fk_dict[robot.tcp_link_name], prismatic_joint)\n",
    "torch_ee_pts = torch_sampler.sample_end_effector(torch_fk[robot.tcp_link_name])\n",
    "\n",
    "print(\"---\" * 10)\n",
    "print(f\"Torch Franka EE points shape: {torch_franka_ee_pts.shape}\")\n",
    "print(f\"Torch EE points shape: {torch_ee_pts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_point(\n",
    "    point_cloud: np.ndarray, point: np.ndarray, tolerance: float = 1e-6\n",
    ") -> tuple[bool, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    exists, idx (bool, np.ndarray): Whether `point` exists in the point cloud\n",
    "        and the indices in the point cloud array where the point exists.\n",
    "    \"\"\"\n",
    "    mask = np.max(np.abs(point_cloud - point), axis=1) <= tolerance\n",
    "    exists = mask.any()\n",
    "    idx = np.flatnonzero(mask)\n",
    "    return exists, idx\n",
    "\n",
    "\n",
    "def compare_point_clouds(\n",
    "    pc1: np.ndarray, pc2: np.ndarray, abs_tol: float = 1e-7\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Return True if input point clouds are identical (within given tolerance)\n",
    "    \"\"\"\n",
    "    # return np.allclose(pc1, pc2, atol=abs_tol)\n",
    "    if np.allclose(pc1, pc2, atol=abs_tol):\n",
    "        return True\n",
    "\n",
    "    print(\"compare_point_clouds() returning False\")\n",
    "    print(\"Largest error:\", np.abs(pc1 - pc2).max())\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ad5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import cprint\n",
    "from robofin.samplers import get_points_on_robot_eef\n",
    "from robofin.old.kinematics.numba import get_points_on_franka_eef, eef_pose_to_link8, franka_eef_link_fk, franka_arm_visual_fk\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# frame = robot.tcp_link_name # right_gripper\n",
    "frame = \"panda_hand\"\n",
    "eef_pose = fk[frame].squeeze()\n",
    "# eef_pose = np.eye(4)\n",
    "eef_fk = robot.eef_fk(eef_pose, frame)\n",
    "\n",
    "lnk8_pose = eef_pose_to_link8(eef_pose, frame)\n",
    "franka_eef_fk_arr = franka_eef_link_fk(prismatic_joint, lnk8_pose)\n",
    "franka_eef_fk = {\n",
    "    \"panda_link8\": franka_eef_fk_arr[0],\n",
    "    \"panda_hand\": franka_eef_fk_arr[1],\n",
    "    \"panda_grasptarget\": franka_eef_fk_arr[2],\n",
    "    \"right_gripper\": franka_eef_fk_arr[3],\n",
    "    \"panda_leftfinger\": franka_eef_fk_arr[4],\n",
    "    \"panda_rightfinger\": franka_eef_fk_arr[5],\n",
    "}\n",
    "# visual_eef_fk = robot.eef_visual_fk(pose, frame, auxiliary_joint_values)\n",
    "\n",
    "if np.allclose(eef_fk[\"panda_link8\"], lnk8_pose):\n",
    "    cprint(\"eef_fk['panda_link8'] == lnk8_pose\", \"green\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "faulty_links = []\n",
    "for link_name in franka_eef_fk:\n",
    "    if np.allclose(eef_fk[link_name], franka_eef_fk[link_name]):\n",
    "        cprint(f\"eef_fk['{link_name}'] EQUALS franka_eef_fk['{link_name}']\", \"cyan\")\n",
    "    else:\n",
    "        cprint(f\"eef_fk['{link_name}'] DOES NOT EQUAL franka_eef_fk['{link_name}']\", \"red\")\n",
    "        faulty_links.append(link_name)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "eef_pts = get_points_on_robot_eef(robot, eef_pose, num_points=0, link_points=np_sampler.points, frame=frame)\n",
    "franka_eef_pts = get_points_on_franka_eef(eef_pose, prismatic_joint=0.04, sample=0, \n",
    "                                          panda_hand_points=np_franka_sampler.points[\"eef_panda_hand\"],\n",
    "                                          panda_leftfinger_points=np_franka_sampler.points[\"eef_panda_leftfinger\"],\n",
    "                                          panda_rightfinger_points=np_franka_sampler.points[\"eef_panda_rightfinger\"],\n",
    "                                          frame=frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import viz_client\n",
    "\n",
    "urdf_path = \"/workspace/assets/panda/panda_spheres.urdf\"\n",
    "if not os.path.exists(urdf_path):\n",
    "    print(f\"❌ URDF not found at {urdf_path}\")\n",
    "    print(\"Please update the urdf_path variable in test_connect()\")\n",
    "    raise FileNotFoundError(f\"URDF not found at {urdf_path}\")\n",
    "\n",
    "viz_client.connect(urdf_path)\n",
    "viz_client.publish_joints(joints=robot.neutral_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_client.publish_ghost_end_effector(pose=fk[\"panda_hand\"], frame=\"panda_hand\", color=[0.8, 0.2, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_client.clear_ghost_end_effector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a037936",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = robot.neutral_config.copy()\n",
    "config[0] = robot.neutral_config[0] + 1.0 \n",
    "viz_client.publish_ghost_robot(config, color=[0.5,0.6,0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f76cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_client.clear_ghost_robot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robofin.old.robot_constants import FrankaConstants, RealFrankaConstants\n",
    "print(FrankaConstants.JOINT_LIMITS[5])\n",
    "print(RealFrankaConstants.JOINT_LIMITS[5])\n",
    "print(robot.main_joint_limits[5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
